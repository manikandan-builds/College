#include <stdio.h>
#include <stdlib.h>
#include <CL/cl.h>
#include <time.h>

const char *kernelSource = 
"__kernel void vec_add(__global int* A, __global int* B, __global int* C) {    \n"
"    int id = get_global_id(0);                                                \n"
"    C[id] = A[id] + B[id];                                                    \n"
"}                                                                             \n";

int main() {
    int n = 1024; // Change this for other sizes
    size_t bytes = n * sizeof(int);

    int *A = (int*)malloc(bytes);
    int *B = (int*)malloc(bytes);
    int *C = (int*)malloc(bytes);

    for (int i = 0; i < n; i++) {
        A[i] = i;
        B[i] = i;
    }

    cl_platform_id platform;
    cl_device_id device;
    cl_context context;
    cl_command_queue queue;
    cl_program program;
    cl_kernel kernel;
    cl_mem d_A, d_B, d_C;

    clGetPlatformIDs(1, &platform, NULL);
    clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device, NULL);
    context = clCreateContext(NULL, 1, &device, NULL, NULL, NULL);
    queue = clCreateCommandQueue(context, device, CL_QUEUE_PROFILING_ENABLE, NULL);

    d_A = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);
    d_B = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, NULL);
    d_C = clCreateBuffer(context, CL_MEM_WRITE_ONLY, bytes, NULL, NULL);

    cl_event evt1, evt2, evt3;

    clEnqueueWriteBuffer(queue, d_A, CL_TRUE, 0, bytes, A, 0, NULL, &evt1);
    clEnqueueWriteBuffer(queue, d_B, CL_TRUE, 0, bytes, B, 0, NULL, &evt2);

    program = clCreateProgramWithSource(context, 1, &kernelSource, NULL, NULL);
    clBuildProgram(program, 1, &device, NULL, NULL, NULL);
    kernel = clCreateKernel(program, "vec_add", NULL);

    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_A);
    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_B);
    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_C);

    size_t globalSize = n;

    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, NULL, 0, NULL, &evt3);
    clEnqueueReadBuffer(queue, d_C, CL_TRUE, 0, bytes, C, 0, NULL, NULL);

    clFinish(queue);

    cl_ulong start1, end1, start2, end2, start3, end3;

    clGetEventProfilingInfo(evt1, CL_PROFILING_COMMAND_START, sizeof(start1), &start1, NULL);
    clGetEventProfilingInfo(evt1, CL_PROFILING_COMMAND_END, sizeof(end1), &end1, NULL);

    clGetEventProfilingInfo(evt2, CL_PROFILING_COMMAND_START, sizeof(start2), &start2, NULL);
    clGetEventProfilingInfo(evt2, CL_PROFILING_COMMAND_END, sizeof(end2), &end2, NULL);

    clGetEventProfilingInfo(evt3, CL_PROFILING_COMMAND_START, sizeof(start3), &start3, NULL);
    clGetEventProfilingInfo(evt3, CL_PROFILING_COMMAND_END, sizeof(end3), &end3, NULL);

    printf("Host to Device Copy A: %.3f ms\n", (end1 - start1) * 1e-6);
    printf("Host to Device Copy B: %.3f ms\n", (end2 - start2) * 1e-6);
    printf("Kernel Execution Time: %.3f ms\n", (end3 - start3) * 1e-6);

    clReleaseMemObject(d_A);
    clReleaseMemObject(d_B);
    clReleaseMemObject(d_C);
    clReleaseProgram(program);
    clReleaseKernel(kernel);
    clReleaseCommandQueue(queue);
    clReleaseContext(context);

    free(A); free(B); free(C);

    return 0;
}
--------------------------------------------------------------------------------------------------------------------
import os
import cv2
import torch
import numpy as np
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from glob import glob

# CNN feature extractor (ResNet18)
cnn_model = models.resnet18(pretrained=True)
cnn_model = torch.nn.Sequential(*list(cnn_model.children())[:-1])  # remove final FC layer
cnn_model.eval()

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def extract_frames(video_path, fps=10):
    cap = cv2.VideoCapture(video_path)
    frames = []
    rate = cap.get(cv2.CAP_PROP_FPS) / fps
    count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % int(rate) == 0:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(transform(frame))
        count += 1
    cap.release()
    return torch.stack(frames)  # Shape: [num_frames, 3, 224, 224]

def extract_features(frames):
    with torch.no_grad():
        features = cnn_model(frames)  # [N, 512, 1, 1]
    return features.squeeze(-1).squeeze(-1)  # [N, 512]
class VideoDataset(Dataset):
    def __init__(self, video_paths, labels):
        self.video_paths = video_paths
        self.labels = labels

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        frames = extract_frames(video_path)
        features = extract_features(frames)  # [N, 512]
        label = self.labels[idx]
        return features, label
import torch.nn as nn

class VideoLSTM(nn.Module):
    def __init__(self, input_size=512, hidden_size=256, num_layers=1, num_classes=10):
        super(VideoLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):  # x: [batch, seq_len, 512]
        out, _ = self.lstm(x)  # [batch, seq_len, hidden]
        out = out[:, -1, :]  # Last timestep
        out = self.fc(out)   # [batch, num_classes]
        return out

def train_model(model, dataloader, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for features, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}")
def evaluate(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for features, labels in dataloader:
            outputs = model(features)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Accuracy: {(100 * correct / total):.2f}%")
# Example video file structure and labels
video_paths = glob("videos/*/*.mp4")  # E.g., videos/action1/vid1.mp4
labels = [int(os.path.basename(os.path.dirname(path))) for path in video_paths]

# Dataset & DataLoader
dataset = VideoDataset(video_paths, labels)
dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))

# Model, Loss, Optimizer
model = VideoLSTM(num_classes=5)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Train & Evaluate
train_model(model, dataloader, criterion, optimizer)
evaluate(model, dataloader)
-------------------------------------------------------------------------------------------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F

class RBM(nn.Module):
    def __init__(self, n_visible=784, n_hidden=256):
        super(RBM, self).__init__()
        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.01)  # weights
        self.h_bias = nn.Parameter(torch.zeros(n_hidden))               # hidden bias
        self.v_bias = nn.Parameter(torch.zeros(n_visible))              # visible bias

    def sample_from_p(self, p):
        return torch.bernoulli(p)

    def v_to_h(self, v):
        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))
        return p_h, self.sample_from_p(p_h)

    def h_to_v(self, h):
        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))
        return p_v, self.sample_from_p(p_v)

    def contrastive_divergence(self, v0, lr=0.01):
        p_h0, h0 = self.v_to_h(v0)
        p_v1, v1 = self.h_to_v(h0)
        p_h1, _ = self.v_to_h(v1)

        self.W.data += lr * (torch.matmul(h0.t(), v0) - torch.matmul(p_h1.t(), v1)) / v0.size(0)
        self.v_bias.data += lr * torch.mean(v0 - v1, dim=0)
        self.h_bias.data += lr * torch.mean(h0 - p_h1, dim=0)

        loss = torch.mean((v0 - v1) ** 2)
        return loss
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1)),  # Flatten to 784
    transforms.Lambda(lambda x: (x > 0.5).float())  # Binarize
])

train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
rbm = RBM(n_visible=784, n_hidden=256)
epochs = 5

for epoch in range(epochs):
    total_loss = 0
    for batch, _ in train_loader:
        loss = rbm.contrastive_divergence(batch)
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}")
import matplotlib.pyplot as plt

def plot_filters(rbm, num_filters=36):
    weights = rbm.W.data.view(-1, 28, 28)
    fig, axes = plt.subplots(6, 6, figsize=(6, 6))
    for i, ax in enumerate(axes.flat):
        if i < num_filters:
            ax.imshow(weights[i], cmap='gray')
            ax.axis('off')
    plt.suptitle("Learned Filters")
    plt.tight_layout()
    plt.show()

plot_filters(rbm)
def generate_image(rbm, k=50):
    v = torch.bernoulli(torch.rand(1, 784))  # Random binary vector
    for _ in range(k):
        _, h = rbm.v_to_h(v)
        _, v = rbm.h_to_v(h)
    return v.view(28, 28)

samples = [generate_image(rbm).detach() for _ in range(16)]

fig, axes = plt.subplots(4, 4, figsize=(6, 6))
for i, ax in enumerate(axes.flat):
    ax.imshow(samples[i], cmap='gray')
    ax.axis('off')
plt.suptitle("Generated Digits via Gibbs Sampling")
plt.tight_layout()
plt.show()
-------------------------------------------------------------------------------------------------------------
import tensorflow as tf
import numpy as np

class RBM(tf.Module):
    def __init__(self, n_visible, n_hidden, name=None):
        super().__init__(name=name)
        self.n_visible = n_visible
        self.n_hidden = n_hidden

        # Weight and biases
        self.W = tf.Variable(tf.random.normal([n_visible, n_hidden], stddev=0.01))
        self.h_bias = tf.Variable(tf.zeros([n_hidden]))
        self.v_bias = tf.Variable(tf.zeros([n_visible]))

    def sample_prob(self, probs):
        return tf.cast(tf.random.uniform(tf.shape(probs)) < probs, tf.float32)

    def v_to_h(self, v):
        h_prob = tf.nn.sigmoid(tf.matmul(v, self.W) + self.h_bias)
        return h_prob, self.sample_prob(h_prob)

    def h_to_v(self, h):
        v_prob = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.W)) + self.v_bias)
        return v_prob, self.sample_prob(v_prob)

    def contrastive_divergence(self, v0, lr=0.01):
        h0_prob, h0_sample = self.v_to_h(v0)
        v1_prob, v1_sample = self.h_to_v(h0_sample)
        h1_prob, _ = self.v_to_h(v1_sample)

        # Update weights and biases
        W_update = tf.matmul(tf.transpose(v0), h0_prob) - tf.matmul(tf.transpose(v1_sample), h1_prob)
        self.W.assign_add(lr * W_update / tf.cast(tf.shape(v0)[0], tf.float32))
        self.v_bias.assign_add(lr * tf.reduce_mean(v0 - v1_sample, axis=0))
        self.h_bias.assign_add(lr * tf.reduce_mean(h0_prob - h1_prob, axis=0))

        loss = tf.reduce_mean(tf.square(v0 - v1_prob))
        return loss

    def forward(self, v):
        h_prob, _ = self.v_to_h(v)
        return h_prob
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

def preprocess(x):
    x = x.astype(np.float32) / 255.
    return x.reshape(-1, 784)

x_train = preprocess(x_train)
x_test = preprocess(x_test)
rbm1 = RBM(784, 512)
rbm2 = RBM(512, 256)

def train_rbm(rbm, data, epochs=5, batch_size=64, lr=0.01):
    for epoch in range(epochs):
        losses = []
        for i in range(0, len(data), batch_size):
            batch = tf.convert_to_tensor(data[i:i+batch_size])
            loss = rbm.contrastive_divergence(batch, lr)
            losses.append(loss.numpy())
        print(f"RBM {rbm.n_visible}->{rbm.n_hidden} | Epoch {epoch+1}, Loss: {np.mean(losses):.4f}")

# Unsupervised Pretraining
print("Pretraining RBM 1...")
train_rbm(rbm1, x_train)

h1_train = rbm1.forward(tf.convert_to_tensor(x_train)).numpy()

print("Pretraining RBM 2...")
train_rbm(rbm2, h1_train)

h2_train = rbm2.forward(tf.convert_to_tensor(h1_train)).numpy()
# Define DBN classifier model
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(784,)),
    tf.keras.layers.Dense(512, activation='sigmoid', weights=[rbm1.W.numpy(), rbm1.h_bias.numpy()]),
    tf.keras.layers.Dense(256, activation='sigmoid', weights=[rbm2.W.numpy(), rbm2.h_bias.numpy()]),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Fine-tuning
model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.1)
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")
